#!/usr/bin/env python3
# Secure LLM CLI â€” Dependency Vulnerability Scanner (TUI)
# - Uses local llama.cpp model (download + encrypt at rest)
# - Scans ONE dependency at a time (Road-Scanner style)
# - Dependency sources:
#   * Local .py file imports (e.g., main.py)
#   * Local project dir (requirements/lockfiles + imports fallback)
#   * GitHub repo URL (shallow clone + parse manifests/locks)

import os
import sys
import time
import json
import shutil
import hashlib
import asyncio
import getpass
import math
import random
import re
import subprocess
import tempfile

import httpx
import aiosqlite

from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, List, Tuple, Callable, Dict

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

from llama_cpp import Llama

try:
    import psutil
except Exception:
    psutil = None

try:
    import pennylane as qml
    from pennylane import numpy as pnp
except Exception:
    qml = None
    pnp = None


# -----------------------------
# Model / storage config
# -----------------------------
MODEL_REPO = "https://huggingface.co/tensorblock/llama3-small-GGUF/resolve/main/"
MODEL_FILE = "llama3-small-Q3_K_M.gguf"
MODELS_DIR = Path("models")
MODEL_PATH = MODELS_DIR / MODEL_FILE
ENCRYPTED_MODEL = MODEL_PATH.with_suffix(MODEL_PATH.suffix + ".aes")

DB_PATH = Path("chat_history.db.aes")
KEY_PATH = Path(".enc_key")

EXPECTED_HASH = "8e4f4856fb84bafb895f1eb08e6c03e4be613ead2d942f91561aeac742a619aa"

MODELS_DIR.mkdir(parents=True, exist_ok=True)


# -----------------------------
# UI helpers
# -----------------------------
CSI = "\x1b["


def clear_screen():
    sys.stdout.write(CSI + "2J" + CSI + "H")
    sys.stdout.flush()


def show_cursor():
    sys.stdout.write(CSI + "?25h")
    sys.stdout.flush()


def color(text, fg=None, bold=False):
    codes = []
    if fg:
        codes.append(str(fg))
    if bold:
        codes.append("1")
    if not codes:
        return text
    return f"\x1b[{';'.join(codes)}m{text}\x1b[0m"


def boxed(title: str, lines: List[str], width: int = 72):
    top = "â”Œ" + "â”€" * (width - 2) + "â”"
    bot = "â””" + "â”€" * (width - 2) + "â”˜"
    title_line = f"â”‚ {color(title, fg=36, bold=True):{width-4}} â”‚"
    body = []
    for l in lines:
        if len(l) > width - 4:
            chunks = [l[i : i + width - 4] for i in range(0, len(l), width - 4)]
        else:
            chunks = [l]
        for c in chunks:
            body.append(f"â”‚ {c:{width-4}} â”‚")
    return "\n".join([top, title_line] + body + [bot])


def getch():
    try:
        import tty, termios

        fd = sys.stdin.fileno()
        old = termios.tcgetattr(fd)
        try:
            tty.setraw(fd)
            ch = os.read(fd, 3)
            return ch
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old)
    except (ImportError, AttributeError, OSError):
        s = input()
        return s[0].encode() if s else b""


def read_menu_choice(num_items: int, prompt="Use â†‘â†“ arrows or number, Enter to select: ") -> int:
    print(prompt)
    try:
        idx = 0
        while True:
            ch = getch()
            if not ch:
                continue
            if ch in (b"\x1b[A", b"\x1b\x00A"):
                idx = (idx - 1) % num_items
            elif ch in (b"\x1b[B", b"\x1b\x00B"):
                idx = (idx + 1) % num_items
            elif ch in (b"\r", b"\n", b"\x0d"):
                return idx
            else:
                try:
                    s = ch.decode(errors="ignore")
                    if s.strip().isdigit():
                        n = int(s.strip())
                        if 1 <= n <= num_items:
                            return n - 1
                except Exception:
                    pass
            sys.stdout.write(f"\rSelected: {idx+1}/{num_items} ")
            sys.stdout.flush()
    except Exception:
        while True:
            s = input("Enter number: ").strip()
            if s.isdigit():
                n = int(s)
                if 1 <= n <= num_items:
                    return n - 1


# -----------------------------
# Crypto helpers
# -----------------------------
def aes_encrypt(data: bytes, key: bytes) -> bytes:
    aes = AESGCM(key)
    nonce = os.urandom(12)
    return nonce + aes.encrypt(nonce, data, None)


def aes_decrypt(data: bytes, key: bytes) -> bytes:
    aes = AESGCM(key)
    nonce, ct = data[:12], data[12:]
    return aes.decrypt(nonce, ct, None)


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def get_or_create_key() -> bytes:
    if KEY_PATH.exists():
        d = KEY_PATH.read_bytes()
        if len(d) >= 48:
            return d[16:48]
        return d[:32]
    key = AESGCM.generate_key(256)
    KEY_PATH.write_bytes(key)
    print(f"ðŸ”‘ New random key generated and saved to {KEY_PATH}")
    return key


def derive_key_from_passphrase(pw: str, salt: Optional[bytes] = None) -> Tuple[bytes, bytes]:
    if salt is None:
        salt = os.urandom(16)
    kdf_der = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=salt, iterations=200_000)
    derived = kdf_der.derive(pw.encode("utf-8"))
    return salt, derived


def ensure_key_interactive() -> bytes:
    if KEY_PATH.exists():
        data = KEY_PATH.read_bytes()
        if len(data) >= 48:
            return data[16:48]
        if len(data) >= 32:
            return data[:32]

    print("Key not found. Create new key:")
    print("  1) Generate random key (saved raw)")
    print("  2) Derive from passphrase (salt+derived saved)")
    opt = input("Choose (1/2): ").strip()
    if opt == "2":
        pw = getpass.getpass("Enter passphrase: ")
        pw2 = getpass.getpass("Confirm: ")
        if pw != pw2:
            print("Passphrases mismatch. Aborting.")
            sys.exit(1)
        salt, key = derive_key_from_passphrase(pw)
        KEY_PATH.write_bytes(salt + key)
        print(f"Saved salt+derived key to {KEY_PATH}")
        return key
    else:
        key = AESGCM.generate_key(256)
        KEY_PATH.write_bytes(key)
        print(f"Saved random key to {KEY_PATH}")
        return key


def download_model_httpx(
    url: str,
    dest: Path,
    show_progress=True,
    timeout=None,
    expected_sha: Optional[str] = None,
):
    print(f"â¬‡ï¸  Downloading model from {url}\nTo: {dest}")
    dest.parent.mkdir(parents=True, exist_ok=True)
    with httpx.stream("GET", url, follow_redirects=True, timeout=timeout) as r:
        r.raise_for_status()
        total = int(r.headers.get("Content-Length") or 0)
        done = 0
        h = hashlib.sha256()
        with dest.open("wb") as f:
            for chunk in r.iter_bytes(chunk_size=8192):
                if not chunk:
                    break
                f.write(chunk)
                h.update(chunk)
                done += len(chunk)
                if total and show_progress:
                    pct = done / total * 100
                    bar = int(pct // 2)
                    sys.stdout.write(
                        f"\r[{('#'*bar).ljust(50)}] {pct:5.1f}% ({done//1024}KB/{total//1024}KB)"
                    )
                    sys.stdout.flush()
    if show_progress:
        print("\nâœ… Download complete.")
    sha = h.hexdigest()
    print(f"SHA256: {sha}")
    if expected_sha:
        if sha.lower() == expected_sha.lower():
            print(color("SHA256 matches expected.", fg=32, bold=True))
        else:
            print(color(f"SHA256 MISMATCH! expected {expected_sha} got {sha}", fg=31, bold=True))
    return sha


def encrypt_file(src: Path, dest: Path, key: bytes):
    print(f"ðŸ” Encrypting {src} -> {dest}")
    data = src.read_bytes()
    start = time.time()
    enc = aes_encrypt(data, key)
    dest.write_bytes(enc)
    dur = time.time() - start
    print(f"âœ… Encrypted ({len(enc)} bytes) in {dur:.2f}s")


def decrypt_file(src: Path, dest: Path, key: bytes):
    print(f"ðŸ”“ Decrypting {src} -> {dest}")
    enc = src.read_bytes()
    data = aes_decrypt(enc, key)
    dest.write_bytes(data)
    print(f"âœ… Decrypted ({len(data)} bytes)")


# -----------------------------
# Encrypted history DB
# -----------------------------
async def init_db(key: bytes):
    if not DB_PATH.exists():
        async with aiosqlite.connect("temp.db") as db:
            await db.execute(
                "CREATE TABLE IF NOT EXISTS history (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp TEXT, prompt TEXT, response TEXT)"
            )
            await db.commit()
        with open("temp.db", "rb") as f:
            enc = aes_encrypt(f.read(), key)
        DB_PATH.write_bytes(enc)
        os.remove("temp.db")


async def log_interaction(prompt: str, response: str, key: bytes):
    dec = Path("temp.db")
    decrypt_file(DB_PATH, dec, key)
    async with aiosqlite.connect(dec) as db:
        await db.execute(
            "INSERT INTO history (timestamp, prompt, response) VALUES (?, ?, ?)",
            (time.strftime("%Y-%m-%d %H:%M:%S"), prompt, response),
        )
        await db.commit()
    with dec.open("rb") as f:
        enc = aes_encrypt(f.read(), key)
    DB_PATH.write_bytes(enc)
    dec.unlink()


async def fetch_history(key: bytes, limit: int = 20, offset: int = 0, search: Optional[str] = None):
    dec = Path("temp.db")
    decrypt_file(DB_PATH, dec, key)
    rows = []
    async with aiosqlite.connect(dec) as db:
        if search:
            q = f"%{search}%"
            async with db.execute(
                "SELECT id,timestamp,prompt,response FROM history WHERE prompt LIKE ? OR response LIKE ? ORDER BY id DESC LIMIT ? OFFSET ?",
                (q, q, limit, offset),
            ) as cur:
                async for r in cur:
                    rows.append(r)
        else:
            async with db.execute(
                "SELECT id,timestamp,prompt,response FROM history ORDER BY id DESC LIMIT ? OFFSET ?",
                (limit, offset),
            ) as cur:
                async for r in cur:
                    rows.append(r)
    with dec.open("rb") as f:
        DB_PATH.write_bytes(aes_encrypt(f.read(), key))
    dec.unlink()
    return rows


# -----------------------------
# Llama loader
# -----------------------------
def load_llama_model_blocking(model_path: Path) -> Llama:
    return Llama(model_path=str(model_path), n_ctx=2048, n_threads=4)


# -----------------------------
# System metrics (entropy)
# -----------------------------
def _read_proc_stat():
    try:
        with open("/proc/stat", "r") as f:
            line = f.readline()
        if not line.startswith("cpu "):
            return None
        parts = line.split()
        vals = [int(x) for x in parts[1:]]
        idle = vals[3] + (vals[4] if len(vals) > 4 else 0)
        total = sum(vals)
        return total, idle
    except Exception:
        return None


def _cpu_percent_from_proc(sample_interval=0.12):
    t1 = _read_proc_stat()
    if not t1:
        return None
    time.sleep(sample_interval)
    t2 = _read_proc_stat()
    if not t2:
        return None
    total1, idle1 = t1
    total2, idle2 = t2
    total_delta = total2 - total1
    idle_delta = idle2 - idle1
    if total_delta <= 0:
        return None
    usage = (total_delta - idle_delta) / float(total_delta)
    return max(0.0, min(1.0, usage))


def _mem_from_proc():
    try:
        info = {}
        with open("/proc/meminfo", "r") as f:
            for line in f:
                parts = line.split(":")
                if len(parts) < 2:
                    continue
                k = parts[0].strip()
                v = parts[1].strip().split()[0]
                info[k] = int(v)

        total = info.get("MemTotal")
        available = info.get("MemAvailable", None)
        if total is None:
            return None
        if available is None:
            available = info.get("MemFree", 0) + info.get("Buffers", 0) + info.get("Cached", 0)
        used_fraction = max(0.0, min(1.0, (total - available) / float(total)))
        return used_fraction
    except Exception:
        return None


def _load1_from_proc(cpu_count_fallback=1):
    try:
        with open("/proc/loadavg", "r") as f:
            first = f.readline().split()[0]
        load1 = float(first)
        try:
            cpu_cnt = os.cpu_count() or cpu_count_fallback
        except Exception:
            cpu_cnt = cpu_count_fallback
        val = load1 / max(1.0, float(cpu_cnt))
        return max(0.0, min(1.0, val))
    except Exception:
        return None


def _proc_count_from_proc():
    try:
        pids = [name for name in os.listdir("/proc") if name.isdigit()]
        return max(0.0, min(1.0, len(pids) / 1000.0))
    except Exception:
        return None


def _read_temperature():
    temps = []
    try:
        base = "/sys/class/thermal"
        if os.path.isdir(base):
            for entry in os.listdir(base):
                if not entry.startswith("thermal_zone"):
                    continue
                path = os.path.join(base, entry, "temp")
                try:
                    with open(path, "r") as f:
                        raw = f.read().strip()
                    if not raw:
                        continue
                    val = int(raw)
                    c = val / 1000.0 if val > 1000 else float(val)
                    temps.append(c)
                except Exception:
                    continue

        if not temps:
            possible = [
                "/sys/devices/virtual/thermal/thermal_zone0/temp",
                "/sys/class/hwmon/hwmon0/temp1_input",
            ]
            for p in possible:
                try:
                    with open(p, "r") as f:
                        raw = f.read().strip()
                    if not raw:
                        continue
                    val = int(raw)
                    c = val / 1000.0 if val > 1000 else float(val)
                    temps.append(c)
                except Exception:
                    continue

        if not temps:
            return None

        avg_c = sum(temps) / len(temps)
        norm = (avg_c - 20.0) / (90.0 - 20.0)
        return max(0.0, min(1.0, norm))
    except Exception:
        return None


def collect_system_metrics() -> Dict[str, float]:
    cpu = mem = load1 = temp = proc = None

    if psutil is not None:
        try:
            cpu = psutil.cpu_percent(interval=0.1) / 100.0
            mem = psutil.virtual_memory().percent / 100.0
            try:
                load_raw = os.getloadavg()[0]
                cpu_cnt = psutil.cpu_count(logical=True) or 1
                load1 = max(0.0, min(1.0, load_raw / max(1.0, float(cpu_cnt))))
            except Exception:
                load1 = None
            try:
                temps_map = psutil.sensors_temperatures()
                if temps_map:
                    first = next(iter(temps_map.values()))[0].current
                    temp = max(0.0, min(1.0, (first - 20.0) / 70.0))
                else:
                    temp = None
            except Exception:
                temp = None
            try:
                proc = min(len(psutil.pids()) / 1000.0, 1.0)
            except Exception:
                proc = None
        except Exception:
            cpu = mem = load1 = temp = proc = None

    if cpu is None:
        cpu = _cpu_percent_from_proc()
    if mem is None:
        mem = _mem_from_proc()
    if load1 is None:
        load1 = _load1_from_proc()
    if proc is None:
        proc = _proc_count_from_proc()
    if temp is None:
        temp = _read_temperature()

    core_ok = all(x is not None for x in (cpu, mem, load1, proc))
    if not core_ok:
        missing = [
            name
            for name, val in (("cpu", cpu), ("mem", mem), ("load1", load1), ("proc", proc))
            if val is None
        ]
        print(f"[FATAL] Unable to obtain core system metrics: missing {missing}")
        sys.exit(2)

    cpu = float(max(0.0, min(1.0, cpu)))
    mem = float(max(0.0, min(1.0, mem)))
    load1 = float(max(0.0, min(1.0, load1)))
    proc = float(max(0.0, min(1.0, proc)))
    temp = float(max(0.0, min(1.0, temp))) if temp is not None else 0.0

    return {"cpu": cpu, "mem": mem, "load1": load1, "temp": temp, "proc": proc}


def metrics_to_rgb(metrics: dict) -> Tuple[float, float, float]:
    cpu = metrics.get("cpu", 0.1)
    mem = metrics.get("mem", 0.1)
    temp = metrics.get("temp", 0.1)
    load1 = metrics.get("load1", 0.0)
    proc = metrics.get("proc", 0.0)
    r = cpu * (1.0 + load1)
    g = mem * (1.0 + proc)
    b = temp * (0.5 + cpu * 0.5)
    maxi = max(r, g, b, 1.0)
    r, g, b = r / maxi, g / maxi, b / maxi
    return (
        float(max(0.0, min(1.0, r))),
        float(max(0.0, min(1.0, g))),
        float(max(0.0, min(1.0, b))),
    )


def pennylane_entropic_score(rgb: Tuple[float, float, float], shots: int = 256) -> float:
    # fallback if pennylane missing
    if qml is None or pnp is None:
        r, g, b = rgb
        ri = max(0, min(255, int(r * 255)))
        gi = max(0, min(255, int(g * 255)))
        bi = max(0, min(255, int(b * 255)))
        seed = (ri << 16) | (gi << 8) | bi
        random.seed(seed)
        base = (0.3 * r + 0.4 * g + 0.3 * b)
        noise = (random.random() - 0.5) * 0.08
        return max(0.0, min(1.0, base + noise))

    dev = qml.device("default.qubit", wires=2, shots=shots)

    @qml.qnode(dev)
    def circuit(a, b, c):
        qml.RX(a * math.pi, wires=0)
        qml.RY(b * math.pi, wires=1)
        qml.CNOT(wires=[0, 1])
        qml.RZ(c * math.pi, wires=1)
        qml.RX((a + b) * math.pi / 2, wires=0)
        qml.RY((b + c) * math.pi / 2, wires=1)
        return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))

    a, b, c = float(rgb[0]), float(rgb[1]), float(rgb[2])

    try:
        ev0, ev1 = circuit(a, b, c)
        combined = ((ev0 + 1.0) / 2.0 * 0.6 + (ev1 + 1.0) / 2.0 * 0.4)
        score = 1.0 / (1.0 + math.exp(-6.0 * (combined - 0.5)))
        return float(max(0.0, min(1.0, score)))
    except Exception:
        return float(0.5 * (a + b + c) / 3.0)


def entropic_summary_text(score: float) -> str:
    if score >= 0.75:
        level = "high"
    elif score >= 0.45:
        level = "medium"
    else:
        level = "low"
    return f"entropic_score={score:.3f} (level={level})"


# -----------------------------
# PUNKD helpers (kept from your style)
# -----------------------------
def _simple_tokenize(text: str) -> List[str]:
    return [t for t in re.findall(r"[A-Za-z0-9_\-]+", text.lower())]


def punkd_analyze(prompt_text: str, top_n: int = 12) -> Dict[str, float]:
    toks = _simple_tokenize(prompt_text)
    freq = {}
    for t in toks:
        freq[t] = freq.get(t, 0) + 1
    # keep your old boost set; it won't hurt
    hazard_boost = {
        "ice": 2.0,
        "wet": 1.8,
        "snow": 2.0,
        "flood": 2.0,
        "construction": 1.8,
        "pedestrian": 1.8,
        "debris": 1.8,
        "animal": 1.5,
        "stall": 1.4,
        "fog": 1.6,
        # dependency-related soft cues
        "malware": 2.0,
        "typosquat": 2.0,
        "backdoor": 2.0,
        "exploit": 1.8,
        "cve": 1.6,
        "vulnerability": 1.6,
        "abandoned": 1.5,
        "suspicious": 1.5,
    }
    scored = {}
    for t, c in freq.items():
        boost = hazard_boost.get(t, 1.0)
        scored[t] = c * boost
    items = sorted(scored.items(), key=lambda x: -x[1])[:top_n]
    if not items:
        return {}
    maxv = items[0][1]
    return {k: float(v / maxv) for k, v in items}


def punkd_apply(prompt_text: str, token_weights: Dict[str, float], profile: str = "balanced") -> Tuple[str, float]:
    if not token_weights:
        return prompt_text, 1.0
    mean_weight = sum(token_weights.values()) / len(token_weights)
    profile_map = {"conservative": 0.6, "balanced": 1.0, "aggressive": 1.4}
    base = profile_map.get(profile, 1.0)
    multiplier = 1.0 + (mean_weight - 0.5) * 0.8 * (base if base > 1.0 else 1.0)
    multiplier = max(0.6, min(1.8, multiplier))
    sorted_tokens = sorted(token_weights.items(), key=lambda x: -x[1])[:6]
    markers = " ".join([f"<ATTN:{t}:{round(w,2)}>" for t, w in sorted_tokens])
    patched = prompt_text + "\n\n[PUNKD_MARKERS] " + markers
    return patched, multiplier


def chunked_generate(
    llm: Llama,
    prompt: str,
    max_total_tokens: int = 256,
    chunk_tokens: int = 64,
    base_temperature: float = 0.2,
    punkd_profile: str = "balanced",
    streaming_callback: Optional[Callable[[str], None]] = None,
) -> str:
    assembled = ""
    cur_prompt = prompt
    token_weights = punkd_analyze(prompt, top_n=16)
    iterations = max(1, (max_total_tokens + chunk_tokens - 1) // chunk_tokens)
    prev_tail = ""

    for _ in range(iterations):
        patched_prompt, mult = punkd_apply(cur_prompt, token_weights, profile=punkd_profile)
        temp = max(0.01, min(2.0, base_temperature * mult))
        out = llm(patched_prompt, max_tokens=chunk_tokens, temperature=temp)

        text = ""
        if isinstance(out, dict):
            try:
                text = out.get("choices", [{"text": ""}])[0].get("text", "")
            except Exception:
                text = out.get("text", "") if isinstance(out, dict) else ""
        else:
            try:
                text = str(out)
            except Exception:
                text = ""

        text = (text or "").strip()
        if not text:
            break

        overlap = 0
        max_ol = min(30, len(prev_tail), len(text))
        for olen in range(max_ol, 0, -1):
            if prev_tail.endswith(text[:olen]):
                overlap = olen
                break

        append_text = text[overlap:] if overlap else text
        assembled += append_text
        prev_tail = assembled[-120:] if len(assembled) > 120 else assembled

        if streaming_callback:
            streaming_callback(append_text)

        # We want one-word outputs for this scanner; stop early when it looks done
        if assembled.strip().endswith(("Low", "Medium", "High")):
            break
        if len(text.split()) < max(4, chunk_tokens // 8):
            break

        cur_prompt = prompt + "\n\nAssistant so far:\n" + assembled + "\n\nContinue:"

    return assembled.strip()


# -----------------------------
# Dependency collection
# -----------------------------
def _normalize_py_req_name(line: str) -> Optional[str]:
    s = line.strip()
    if not s or s.startswith("#"):
        return None
    # skip includes/options
    if s.startswith(("-r ", "--requirement", "-c ", "--constraint", "--index-url", "--extra-index-url", "--find-links")):
        return None
    s = s.split("#", 1)[0].strip()
    if not s:
        return None
    s = s.split(";", 1)[0].strip()
    if "://" in s or s.startswith(("-e ", "git+")):
        return None
    s = re.split(r"\[", s, 1)[0].strip()
    s = re.split(r"(==|>=|<=|~=|!=|>|<)", s, 1)[0].strip()
    s = s.strip()
    if not s:
        return None
    if not re.match(r"^[A-Za-z0-9_.\-]+$", s):
        return None
    return s


def extract_imports_from_py_text(text: str) -> List[str]:
    pkgs = set()
    # crude stdlib exclusions (keeps noise down)
    stdish = {
        "os", "sys", "re", "json", "math", "time", "typing", "pathlib", "asyncio",
        "threading", "hashlib", "subprocess", "tempfile", "shutil", "itertools",
        "functools", "collections", "dataclasses", "logging", "inspect", "base64",
        "unittest", "doctest", "argparse", "enum", "signal", "socket"
    }
    for line in text.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue

        m1 = re.match(r"^\s*import\s+(.+)$", line)
        if m1:
            rest = m1.group(1)
            parts = [p.strip() for p in rest.split(",")]
            for p in parts:
                root = p.split(" as ", 1)[0].strip()
                root = root.split(".", 1)[0].strip()
                if root and root not in stdish:
                    pkgs.add(root)
            continue

        m2 = re.match(r"^\s*from\s+([A-Za-z0-9_\.]+)\s+import\s+.+$", line)
        if m2:
            root = m2.group(1).split(".", 1)[0].strip()
            if root and root not in stdish:
                pkgs.add(root)

    return sorted(pkgs)


def extract_imports_from_file(path: Path) -> List[str]:
    try:
        text = path.read_text(errors="ignore")
    except Exception:
        return []
    return extract_imports_from_py_text(text)


def parse_requirements_txt(path: Path) -> List[str]:
    pkgs = set()
    try:
        for line in path.read_text(errors="ignore").splitlines():
            name = _normalize_py_req_name(line)
            if name:
                pkgs.add(name)
    except Exception:
        pass
    return sorted(pkgs)


def parse_pipfile_lock(path: Path) -> List[str]:
    pkgs = set()
    try:
        data = json.loads(path.read_text(errors="ignore"))
        for section in ("default", "develop"):
            deps = data.get(section, {}) or {}
            if isinstance(deps, dict):
                for name in deps.keys():
                    if isinstance(name, str) and name.strip():
                        pkgs.add(name.strip())
    except Exception:
        pass
    return sorted(pkgs)


def parse_poetry_lock(path: Path) -> List[str]:
    pkgs = set()
    try:
        txt = path.read_text(errors="ignore")
        for m in re.finditer(r'^\s*name\s*=\s*"([^"]+)"\s*$', txt, flags=re.M):
            pkgs.add(m.group(1).strip())
    except Exception:
        pass
    return sorted(pkgs)


def parse_package_json(path: Path) -> List[str]:
    pkgs = set()
    try:
        data = json.loads(path.read_text(errors="ignore"))
        for k in ("dependencies", "devDependencies", "peerDependencies", "optionalDependencies"):
            deps = data.get(k, {}) or {}
            if isinstance(deps, dict):
                for name in deps.keys():
                    if isinstance(name, str) and name.strip():
                        pkgs.add(name.strip())
    except Exception:
        pass
    return sorted(pkgs)


def parse_package_lock_json(path: Path) -> List[str]:
    pkgs = set()
    try:
        data = json.loads(path.read_text(errors="ignore"))
        deps = data.get("dependencies", {}) or {}
        if isinstance(deps, dict):
            for name in deps.keys():
                if isinstance(name, str) and name.strip():
                    pkgs.add(name.strip())

        packages_obj = data.get("packages", {}) or {}
        if isinstance(packages_obj, dict):
            for k in packages_obj.keys():
                if isinstance(k, str) and k.startswith("node_modules/"):
                    name = k[len("node_modules/") :].strip()
                    if name:
                        pkgs.add(name)
    except Exception:
        pass
    return sorted(pkgs)


def parse_yarn_lock(path: Path) -> List[str]:
    pkgs = set()
    try:
        for line in path.read_text(errors="ignore").splitlines():
            s = line.strip()
            if not s or s.startswith("#"):
                continue
            if s.endswith(":") and (s.startswith('"') or s.startswith("'")):
                key = s[:-1].strip().strip('"').strip("'")
                first = key.split(",")[0].strip()
                # handle scoped packages: @scope/name@^1.2.3
                if first.startswith("@"):
                    at = first.rfind("@")
                    name = first[:at] if at > 0 else first
                    pkgs.add(name)
                else:
                    name = first.split("@", 1)[0].strip()
                    if name:
                        pkgs.add(name)
    except Exception:
        pass
    return sorted(pkgs)


def parse_pnpm_lock_yaml(path: Path) -> List[str]:
    pkgs = set()
    try:
        txt = path.read_text(errors="ignore")
        in_top_deps = False
        for line in txt.splitlines():
            if re.match(r"^\s*dependencies:\s*$", line) or re.match(r"^\s*devDependencies:\s*$", line):
                in_top_deps = True
                continue
            # other top-level sections stop parsing top deps
            if re.match(r"^[A-Za-z].*:\s*$", line) and not line.startswith("  "):
                in_top_deps = False

            if in_top_deps:
                m = re.match(r"^\s{2}(@?[\w\-.]+\/?[\w\-.]*):\s+", line)
                if m:
                    name = m.group(1).strip()
                    if name:
                        pkgs.add(name)
    except Exception:
        pass
    return sorted(pkgs)


def clone_github_repo(url: str) -> Path:
    tmpdir = Path(tempfile.mkdtemp(prefix="repo_"))
    subprocess.run(["git", "clone", "--depth=1", url, str(tmpdir)], check=True)
    return tmpdir


def infer_ecosystem_from_repo(root: Path) -> str:
    if (root / "package.json").exists():
        return "JavaScript"
    if (root / "requirements.txt").exists() or (root / "pyproject.toml").exists() or any(root.rglob("*.py")):
        return "Python"
    return "Python"


def collect_dependencies_from_target(target: str) -> Tuple[List[str], str, str, Optional[Path]]:
    """
    Returns: (packages, ecosystem, label_name, temp_repo_path_if_any)
    """
    tmp_repo = None
    p = Path(target)

    # GitHub URL
    if target.startswith(("http://", "https://")) and "github.com" in target.lower():
        tmp_repo = clone_github_repo(target)
        label_name = target.rstrip("/").split("/")[-1] or "repo"
        ecosystem = infer_ecosystem_from_repo(tmp_repo)
        packages = set()

        if ecosystem.lower() == "python":
            if (tmp_repo / "requirements.txt").exists():
                packages |= set(parse_requirements_txt(tmp_repo / "requirements.txt"))
            if (tmp_repo / "Pipfile.lock").exists():
                packages |= set(parse_pipfile_lock(tmp_repo / "Pipfile.lock"))
            if (tmp_repo / "poetry.lock").exists():
                packages |= set(parse_poetry_lock(tmp_repo / "poetry.lock"))

            main_py = tmp_repo / "main.py"
            if main_py.exists():
                packages |= set(extract_imports_from_file(main_py))
            else:
                for pyf in tmp_repo.rglob("*.py"):
                    packages |= set(extract_imports_from_file(pyf))

        else:
            if (tmp_repo / "package.json").exists():
                packages |= set(parse_package_json(tmp_repo / "package.json"))
            if (tmp_repo / "package-lock.json").exists():
                packages |= set(parse_package_lock_json(tmp_repo / "package-lock.json"))
            if (tmp_repo / "yarn.lock").exists():
                packages |= set(parse_yarn_lock(tmp_repo / "yarn.lock"))
            if (tmp_repo / "pnpm-lock.yaml").exists():
                packages |= set(parse_pnpm_lock_yaml(tmp_repo / "pnpm-lock.yaml"))

        return sorted(packages), ecosystem, label_name, tmp_repo

    # Local file
    if p.exists() and p.is_file():
        label_name = p.stem
        if p.suffix.lower() == ".py":
            return extract_imports_from_file(p), "Python", label_name, None

        if p.suffix.lower() in (".js", ".ts", ".tsx"):
            txt = p.read_text(errors="ignore")
            pkgs = set()
            for m in re.finditer(r'from\s+["\']([^"\']+)["\']', txt):
                mod = m.group(1)
                if not mod.startswith((".", "/")):
                    pkgs.add(mod if mod.startswith("@") else mod.split("/")[0])
            for m in re.finditer(r'require\(\s*["\']([^"\']+)["\']\s*\)', txt):
                mod = m.group(1)
                if not mod.startswith((".", "/")):
                    pkgs.add(mod if mod.startswith("@") else mod.split("/")[0])
            return sorted(pkgs), "JavaScript", label_name, None

        return [], "Python", label_name, None

    # Local directory project
    if p.exists() and p.is_dir():
        label_name = p.name
        ecosystem = infer_ecosystem_from_repo(p)
        packages = set()

        if ecosystem.lower() == "python":
            if (p / "requirements.txt").exists():
                packages |= set(parse_requirements_txt(p / "requirements.txt"))
            if (p / "Pipfile.lock").exists():
                packages |= set(parse_pipfile_lock(p / "Pipfile.lock"))
            if (p / "poetry.lock").exists():
                packages |= set(parse_poetry_lock(p / "poetry.lock"))

            main_py = p / "main.py"
            if main_py.exists():
                packages |= set(extract_imports_from_file(main_py))
            else:
                for pyf in p.rglob("*.py"):
                    packages |= set(extract_imports_from_file(pyf))

        else:
            if (p / "package.json").exists():
                packages |= set(parse_package_json(p / "package.json"))
            if (p / "package-lock.json").exists():
                packages |= set(parse_package_lock_json(p / "package-lock.json"))
            if (p / "yarn.lock").exists():
                packages |= set(parse_yarn_lock(p / "yarn.lock"))
            if (p / "pnpm-lock.yaml").exists():
                packages |= set(parse_pnpm_lock_yaml(p / "pnpm-lock.yaml"))

        return sorted(packages), ecosystem, label_name, None

    return [], "Python", "unknown_target", None


# -----------------------------
# Vuln scanner prompt (single dependency, single-word output)
# -----------------------------
def build_dependency_vuln_prompt(dep_name: str, ecosystem: str, include_system_entropy: bool = True) -> str:
    entropy_text = "entropic_score=unknown"
    if include_system_entropy:
        metrics = collect_system_metrics()
        rgb = metrics_to_rgb(metrics)
        score = pennylane_entropic_score(rgb)
        entropy_text = entropic_summary_text(score)
        metrics_line = "sys_metrics: cpu={cpu:.2f},mem={mem:.2f},load={load1:.2f},temp={temp:.2f},proc={proc:.2f}".format(
            cpu=metrics.get("cpu", 0.0),
            mem=metrics.get("mem", 0.0),
            load1=metrics.get("load1", 0.0),
            temp=metrics.get("temp", 0.0),
            proc=metrics.get("proc", 0.0),
        )
    else:
        metrics_line = "sys_metrics: disabled"

    ecosystem_note = "Python (pip)" if ecosystem.lower() == "python" else "JavaScript/TypeScript (npm)"

    tpl = (
        f"You are a Hypertime Nanobot specialized Dependency Vulnerability Risk Classifier.\n"
        f"Your task is to assess supply-chain and vulnerability risk for ONE dependency.\n"
        f"Your reply must be only one word: Low, Medium, or High.\n\n"
        f"[tuning]\n"
        f"Ecosystem: {ecosystem_note}\n"
        f"Dependency: {dep_name}\n"
        f"{metrics_line}\n"
        f"Quantum State: {entropy_text}\n"
        f"[/tuning]\n\n"
        f"Strict classification guidelines:\n"
        f"- Low: widely used, well maintained, reputable publisher, no known critical issues.\n"
        f"- Medium: moderate popularity or unclear maintenance, occasional issues, uncertain provenance.\n"
        f"- High: suspicious origin, typosquat-like, abandoned with known issues, frequent severe vulns, or clearly risky.\n\n"
        f"Rules:\n"
        f"- Think internally but show NO reasoning.\n"
        f"- Use the entropic signal to slightly bias confidence if needed.\n"
        f"- If unknown, default to Medium.\n"
        f"- Output exactly one word, with no punctuation or labels.\n\n"
        f"[replytemplate]\nLow | Medium | High\n[/replytemplate]\n"
        f"Output exactly one word now:"
    )
    return tpl


# -----------------------------
# App header / common
# -----------------------------
def header(status: dict):
    s = f" Secure LLM CLI â€” Model: {'loaded' if status.get('model_loaded') else 'none'} | Key: {'present' if status.get('key') else 'missing'} "
    print(color(s.center(80, "â”€"), fg=35, bold=True))


# -----------------------------
# Model Manager
# -----------------------------
def model_manager(state: dict):
    while True:
        clear_screen()
        header(state)
        lines = [
            "1) Download model from remote repo (httpx)",
            "2) Verify plaintext model hash (compute SHA256)",
            "3) Encrypt plaintext model -> .aes",
            "4) Decrypt .aes -> plaintext (temporary)",
            "5) Delete plaintext model",
            "6) Back",
        ]
        print(boxed("Model Manager", lines))
        choice = input("Choose (1-6): ").strip()

        if choice == "1":
            if MODEL_PATH.exists():
                if input("Plaintext model exists; overwrite? (y/N): ").strip().lower() != "y":
                    continue
            try:
                url = MODEL_REPO + MODEL_FILE
                sha = download_model_httpx(url, MODEL_PATH, show_progress=True, timeout=None, expected_sha=EXPECTED_HASH)
                print(f"Downloaded to {MODEL_PATH}")
                print(f"Computed SHA256: {sha}")
                if input("Encrypt downloaded model with current key now? (Y/n): ").strip().lower() != "n":
                    encrypt_file(MODEL_PATH, ENCRYPTED_MODEL, state["key"])
                    print(f"Encrypted -> {ENCRYPTED_MODEL}")
                    if input("Remove plaintext model? (Y/n): ").strip().lower() != "n":
                        MODEL_PATH.unlink()
                        print("Plaintext removed.")
            except Exception as e:
                print(f"Download failed: {e}")
            input("Enter to continue...")

        elif choice == "2":
            if not MODEL_PATH.exists():
                print("No plaintext model found.")
            else:
                print(f"SHA256: {sha256_file(MODEL_PATH)}")
            input("Enter to continue...")

        elif choice == "3":
            if not MODEL_PATH.exists():
                print("No plaintext model to encrypt.")
                input("Enter...")
                continue
            encrypt_file(MODEL_PATH, ENCRYPTED_MODEL, state["key"])
            if input("Remove plaintext? (Y/n): ").strip().lower() != "n":
                MODEL_PATH.unlink()
                print("Removed plaintext.")
            input("Enter...")

        elif choice == "4":
            if not ENCRYPTED_MODEL.exists():
                print("No .aes model present.")
            else:
                decrypt_file(ENCRYPTED_MODEL, MODEL_PATH, state["key"])
            input("Enter...")

        elif choice == "5":
            if MODEL_PATH.exists():
                if input(f"Delete {MODEL_PATH}? (y/N): ").strip().lower() == "y":
                    MODEL_PATH.unlink()
                    print("Deleted.")
            else:
                print("No plaintext model.")
            input("Enter...")

        elif choice == "6":
            return
        else:
            print("Invalid.")


# -----------------------------
# Chat session (kept)
# -----------------------------
async def chat_session(state: dict):
    if not ENCRYPTED_MODEL.exists():
        print("No encrypted model found. Please download & encrypt first.")
        input("Enter...")
        return

    decrypt_file(ENCRYPTED_MODEL, MODEL_PATH, state["key"])
    loop = asyncio.get_running_loop()

    with ThreadPoolExecutor(max_workers=1) as ex:
        try:
            print("Loading model...")
            llm = await loop.run_in_executor(ex, load_llama_model_blocking, MODEL_PATH)
        except Exception as e:
            print(f"Failed to load: {e}")
            if MODEL_PATH.exists():
                try:
                    encrypt_file(MODEL_PATH, ENCRYPTED_MODEL, state["key"])
                    MODEL_PATH.unlink()
                except Exception:
                    pass
            input("Enter...")
            return

        state["model_loaded"] = True
        try:
            await init_db(state["key"])
            print("Type /exit to return, /history to show last 10 messages.")
            while True:
                prompt = input("\nYou> ").strip()
                if not prompt:
                    continue
                if prompt in ("/exit", "exit", "quit"):
                    break
                if prompt == "/history":
                    rows = await fetch_history(state["key"], limit=10)
                    for r in rows:
                        print(f"[{r[0]}] {r[1]}\nQ: {r[2]}\nA: {r[3]}\n{'-'*30}")
                    continue

                def gen(p):
                    out = llm(p, max_tokens=256, temperature=0.7)
                    text = ""
                    if isinstance(out, dict):
                        try:
                            text = out.get("choices", [{"text": ""}])[0].get("text", "")
                        except Exception:
                            text = out.get("text", "")
                    else:
                        text = str(out)
                    text = (text or "").strip()
                    text = text.replace("You are a helpful AI assistant named SmolLM, trained by Hugging Face", "").strip()
                    return text

                print("ðŸ¤– Thinking...")
                result = await loop.run_in_executor(ex, gen, prompt)
                print("\nModel:\n" + result + "\n")
                await log_interaction(prompt, result, state["key"])

        finally:
            try:
                del llm
            except Exception:
                pass
            print("Re-encrypting model and removing plaintext...")
            try:
                encrypt_file(MODEL_PATH, ENCRYPTED_MODEL, state["key"])
                MODEL_PATH.unlink()
                state["model_loaded"] = False
            except Exception as e:
                print(f"Cleanup failed: {e}")
            input("Enter...")


# -----------------------------
# Dependency Vuln Scanner (ONE dependency at a time)
# -----------------------------
async def dependency_vuln_scanner_flow(state: dict):
    clear_screen()
    header(state)

    print(
        boxed(
            "Dependency Vuln Scanner - Input",
            [
                "Paste a GitHub repo URL OR a local path:",
                "- GitHub: https://github.com/user/repo",
                "- Local file: /path/to/main.py",
                "- Local dir:  /path/to/project (reads requirements/lock files, etc.)",
                "",
                "Behavior: scans ONE dependency at a time (Road-Scanner style).",
            ],
        )
    )

    target = input("Target: ").strip()
    if not target:
        print("No input.")
        input("Enter...")
        return

    tmp_repo = None
    try:
        packages, ecosystem, label_name, tmp_repo = collect_dependencies_from_target(target)
        packages = [p for p in packages if isinstance(p, str) and p.strip()]
        packages = sorted(set(packages))

        if not packages:
            print("No dependencies detected (imports/lockfiles empty).")
            input("Enter...")
            return

        print("\nGeneration options:\n1) Chunked generation + punkd (recommended)\n2) Chunked only\n3) Direct single-call generation")
        gen_choice = input("Choose (1-3) [1]: ").strip() or "1"
        punkd_profile = "balanced" if gen_choice == "1" else "conservative"

        if not ENCRYPTED_MODEL.exists():
            print("No encrypted model found. Please download & encrypt first.")
            input("Enter...")
            return

        decrypt_file(ENCRYPTED_MODEL, MODEL_PATH, state["key"])
        loop = asyncio.get_running_loop()

        with ThreadPoolExecutor(max_workers=1) as ex:
            try:
                llm = await loop.run_in_executor(ex, load_llama_model_blocking, MODEL_PATH)
            except Exception as e:
                print(f"Model load failed: {e}")
                input("Enter...")
                return

            results: Dict[str, str] = {}
            idx = 0

            def gen_direct(p: str) -> str:
                out = llm(p, max_tokens=96, temperature=0.2)
                if isinstance(out, dict):
                    try:
                        text = out.get("choices", [{"text": ""}])[0].get("text", "")
                    except Exception:
                        text = out.get("text", "")
                else:
                    text = str(out)
                return (text or "").strip().replace(
                    "You are a helpful AI assistant named SmolLM, trained by Hugging Face", ""
                ).strip()

            def run_scan_one(dep: str) -> str:
                prompt = build_dependency_vuln_prompt(dep, ecosystem=ecosystem, include_system_entropy=True)
                if gen_choice == "3":
                    return gen_direct(prompt)
                return chunked_generate(
                    llm=llm,
                    prompt=prompt,
                    max_total_tokens=128,
                    chunk_tokens=64,
                    base_temperature=0.18,
                    punkd_profile=punkd_profile,
                    streaming_callback=None,
                )

            def parse_label(text: str) -> str:
                t = (text or "").strip()
                cand = t.split()
                label = cand[0].capitalize() if cand else ""
                if label not in ("Low", "Medium", "High"):
                    lowered = t.lower()
                    if "low" in lowered:
                        label = "Low"
                    elif "medium" in lowered:
                        label = "Medium"
                    elif "high" in lowered:
                        label = "High"
                    else:
                        label = "Medium"
                return label

            while True:
                clear_screen()
                header(state)

                dep = packages[idx]
                scanned_label = results.get(dep)

                lines = [
                    f"Target: {label_name}",
                    f"Ecosystem: {ecosystem}",
                    f"Dependency {idx+1}/{len(packages)}: {dep}",
                    f"Last result: {scanned_label or '(not scanned)'}",
                    "",
                    "Commands:",
                    "  s = scan this dep (default Enter)",
                    "  n = next dep, p = prev dep",
                    "  a = scan ALL deps (one-by-one)",
                    "  e = export report JSON",
                    "  q = quit",
                ]
                print(boxed("Dependency Vuln Scanner", lines))

                cmd = input("cmd [s]: ").strip().lower() or "s"

                if cmd == "q":
                    break
                if cmd == "n":
                    idx = (idx + 1) % len(packages)
                    continue
                if cmd == "p":
                    idx = (idx - 1) % len(packages)
                    continue
                if cmd == "e":
                    fn = f"dep_vuln_report_{label_name}.json".replace(" ", "_")
                    outp = {
                        "target": label_name,
                        "ecosystem": ecosystem,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "results": results,
                        "total_deps": len(packages),
                    }
                    Path(fn).write_text(json.dumps(outp, indent=2))
                    print(f"Saved {fn}")
                    input("Enter...")
                    continue

                if cmd == "a":
                    print("Scanning ALL dependencies one-by-one...\n")
                    for j, depj in enumerate(packages):
                        print(f"[{j+1}/{len(packages)}] {depj} ... ", end="", flush=True)
                        raw = await loop.run_in_executor(ex, run_scan_one, depj)
                        label = parse_label(raw)
                        results[depj] = label
                        print(label)
                    input("\nDone. Enter...")
                    continue

                # scan current dep
                print("Scanning...\n")
                raw = await loop.run_in_executor(ex, run_scan_one, dep)
                label = parse_label(raw)
                results[dep] = label

                print("\n--- Dependency Scan Result ---\n")
                fg = 32 if label == "Low" else 33 if label == "Medium" else 31
                print(color(f"{dep}: {label}", fg=fg, bold=True))

                lows = sum(1 for v in results.values() if v == "Low")
                meds = sum(1 for v in results.values() if v == "Medium")
                highs = sum(1 for v in results.values() if v == "High")
                print(f"\nReport so far: Low={lows} Medium={meds} High={highs} (scanned {len(results)}/{len(packages)})")

                nxt = input("\nNext? (n=next, p=prev, e=export, Enter=menu): ").strip().lower()
                if nxt == "n":
                    idx = (idx + 1) % len(packages)
                elif nxt == "p":
                    idx = (idx - 1) % len(packages)
                elif nxt == "e":
                    fn = f"dep_vuln_report_{label_name}.json".replace(" ", "_")
                    outp = {
                        "target": label_name,
                        "ecosystem": ecosystem,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "results": results,
                        "total_deps": len(packages),
                    }
                    Path(fn).write_text(json.dumps(outp, indent=2))
                    print(f"Saved {fn}")
                    input("Enter...")

            # log summary
            try:
                await init_db(state["key"])
                await log_interaction(
                    f"DEP_VULN_SCAN_TARGET:\n{target}\nEcosystem={ecosystem}\nDeps={len(packages)}",
                    f"DEP_VULN_SCAN_RESULTS:\n{json.dumps(results, indent=2)}",
                    state["key"],
                )
            except Exception:
                pass

            try:
                del llm
            except Exception:
                pass

    except subprocess.CalledProcessError as e:
        print(f"Git clone failed: {e}")
        input("Enter...")

    except FileNotFoundError as e:
        # commonly: git not installed
        print(f"System tool missing: {e}")
        print("Tip: install git, or pass a local path instead of a GitHub URL.")
        input("Enter...")

    except Exception as e:
        print(f"Error: {e}")
        input("Enter...")

    finally:
        if tmp_repo and tmp_repo.exists():
            shutil.rmtree(tmp_repo, ignore_errors=True)

        try:
            if MODEL_PATH.exists():
                print("Re-encrypting model and removing plaintext...")
                encrypt_file(MODEL_PATH, ENCRYPTED_MODEL, state["key"])
                MODEL_PATH.unlink()
        except Exception as e:
            print(f"Cleanup error: {e}")

        show_cursor()


# -----------------------------
# DB viewer / Rekey (kept)
# -----------------------------
async def db_viewer_flow(state: dict):
    if not DB_PATH.exists():
        print("No DB found.")
        input("Enter...")
        return
    page = 0
    per_page = 10
    search = None
    while True:
        rows = await fetch_history(state["key"], limit=per_page, offset=page * per_page, search=search)
        clear_screen()
        header(state)
        title = f"History (page {page+1})"
        print(boxed(title, [f"Search: {search or '(none)'}", "Commands: n=next p=prev s=search q=quit"]))
        if not rows:
            print("No rows on this page.")
        else:
            for r in rows:
                print(f"[{r[0]}] {r[1]}\nQ: {r[2]}\nA: {r[3]}\n" + "-" * 60)
        cmd = input("cmd (n/p/s/q): ").strip().lower()
        if cmd == "n":
            page += 1
        elif cmd == "p" and page > 0:
            page -= 1
        elif cmd == "s":
            search = input("Enter search keyword (empty to clear): ").strip() or None
            page = 0
        else:
            break


def safe_cleanup(paths: List[Path]):
    for p in paths:
        try:
            if p.exists():
                p.unlink()
        except Exception:
            pass


def rekey_flow(state: dict):
    print("Rekey / Rotate Key")
    if KEY_PATH.exists():
        print(f"Current key file: {KEY_PATH}")
    else:
        print("No existing key file (creating new).")

    choice = input("1) New random key  2) Passphrase-derived  3) Cancel\nChoose: ").strip()
    if choice not in ("1", "2"):
        print("Canceled.")
        input("Enter...")
        return

    old_key = state["key"]
    tmp_model = MODELS_DIR / (MODEL_FILE + ".tmp")
    tmp_db = Path("temp.db")

    try:
        if ENCRYPTED_MODEL.exists():
            try:
                decrypt_file(ENCRYPTED_MODEL, tmp_model, old_key)
            except Exception as e:
                print(f"Failed to decrypt model with current key: {e}")
                safe_cleanup([tmp_model, tmp_db])
                input("Enter...")
                return

        if DB_PATH.exists():
            try:
                decrypt_file(DB_PATH, tmp_db, old_key)
            except Exception as e:
                print(f"Failed to decrypt DB with current key: {e}")
                safe_cleanup([tmp_model, tmp_db])
                input("Enter...")
                return

    except Exception as e:
        print(f"Unexpected: {e}")
        safe_cleanup([tmp_model, tmp_db])
        input("Enter...")
        return

    if choice == "1":
        new_key = AESGCM.generate_key(256)
        KEY_PATH.write_bytes(new_key)
        print("New random key generated and saved.")
    else:
        pw = getpass.getpass("Enter new passphrase: ")
        pw2 = getpass.getpass("Confirm: ")
        if pw != pw2:
            print("Mismatch.")
            safe_cleanup([tmp_model, tmp_db])
            input("Enter...")
            return
        salt, derived = derive_key_from_passphrase(pw)
        KEY_PATH.write_bytes(salt + derived)
        new_key = derived
        print("New passphrase-derived key saved (salt+derived).")

    try:
        if tmp_model.exists():
            old_h = sha256_file(tmp_model)
            encrypt_file(tmp_model, ENCRYPTED_MODEL, new_key)
            new_h_enc = sha256_file(ENCRYPTED_MODEL)
            print(f"Model plaintext SHA256: {old_h}")
            print(f"Encrypted model SHA256: {new_h_enc}")

        if tmp_db.exists():
            old_db_h = sha256_file(tmp_db)
            with tmp_db.open("rb") as f:
                DB_PATH.write_bytes(aes_encrypt(f.read(), new_key))
            new_db_h = sha256_file(DB_PATH)
            print(f"DB plaintext SHA256: {old_db_h}")
            print(f"Encrypted DB SHA256: {new_db_h}")

    except Exception as e:
        print(f"Error during re-encryption: {e}")

    finally:
        safe_cleanup([tmp_model, tmp_db])
        raw = KEY_PATH.read_bytes() if KEY_PATH.exists() else b""
        state["key"] = raw[16:48] if len(raw) >= 48 else raw[:32]
        print("Rekey attempt finished. Verify files manually.")
        input("Enter...")


# -----------------------------
# Main menu / entry
# -----------------------------
def main_menu_loop(state: dict):
    options = [
        "Model Manager",
        "Chat with model",
        "Vulnerability Scanner",
        "View chat history",
        "Rekey / Rotate key",
        "Exit",
    ]
    while True:
        clear_screen()
        header(state)
        print()
        print(boxed("Main Menu", [f"{i+1}) {opt}" for i, opt in enumerate(options)]))
        idx = read_menu_choice(len(options))
        choice = options[idx]

        if choice == "Model Manager":
            model_manager(state)
        elif choice == "Chat with model":
            asyncio.run(chat_session(state))
        elif choice == "Vulnerability Scanner":
            asyncio.run(dependency_vuln_scanner_flow(state))
        elif choice == "View chat history":
            asyncio.run(db_viewer_flow(state))
        elif choice == "Rekey / Rotate key":
            rekey_flow(state)
        elif choice == "Exit":
            print("Goodbye.")
            return


def main():
    try:
        key = ensure_key_interactive()
    except Exception:
        key = get_or_create_key()

    state = {"key": key, "model_loaded": False}

    try:
        asyncio.run(init_db(state["key"]))
    except Exception:
        pass

    try:
        main_menu_loop(state)
    except KeyboardInterrupt:
        print("\nInterrupted.")
    finally:
        show_cursor()


if __name__ == "__main__":
    main()
